---
title: "A2_Group5"
author: "Merel Das (6600034), Mirre Dona (6156630), Lukas Stemerdink (6217265)"
date: "2023-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries

```{r, message=FALSE}
library(ISLR) 
library(tidyverse) 
library(readr) # For data import
library(dplyr)
library(magrittr) # For pipes
library(mice) # For missing data
library(ggplot2) # For plotting
library(psych)
library(reshape2)

library(randomForest) # For random forest
library(rpart) # For decision tree
library(rpart.plot) # For plotting decision tree
library(caret) # rf and boosting

library(gridExtra) # For assumption checking of LDA
library(caret) # For confusion matrix

# install.packages("Metrics")
library(Metrics) # For accuracy metric

```

# Import dataset
```{r}
sleepdata <- read.csv('data/Sleep_health_and_lifestyle_dataset.csv')
head(sleepdata)
```

# Description of the data
The Sleep Health and Lifestyle Dataset, here called sleepdata, consists of 400 rows and 13 columns. The dataset consists of sleep-related data. A lot of variables are presented, including gender, age, occupation, sleep duration, quality of sleep, physical activity levels, stress levels, BMI category, blood pressure, heart rate, daily steps, and, if it is the case, the sleep disorders. This dataset offers the opportunity to gain insights into whether sleep factors and lifestyle factors are related.

The following variables are included in the dataset:

* **Person ID**: An identifier for each individual in the dataset (stored as integer).
* **Gender**: [Male/Female] (stored as string).
* **Age**: The age of the person in years (stored as integer).
* **Occupation**: The occupation or profession of the person (stored as string).
* **Sleep Duration**: The number of hours the person sleeps per day (stored as numeric).
* **Quality of Sleep**: [1-10] A subjective rating of the quality of sleep, ranging from 1 to 10 (stored as integer).
* **Physical Activity Level**: The number of minutes the person engages in physical activity daily (stored as integer of minutes/day).
* **Stress Level** [1-10]: A subjective rating of the stress level experienced by the person, ranging from 1 to 10 (stored as integer).
* **BMI Category**: The BMI category of the person (e.g., Underweight, Normal, Overweight, stored as string).
* **Blood Pressure** (systolic/diastolic): The blood pressure measurement of the person, indicated as systolic pressure over diastolic pressure (stored as string).
* **Heart Rate** (bpm): The resting heart rate of the person in beats per minute (stored as integer).
* **Daily Steps**: The number of steps the person takes per day (stored as integer).
* **Sleep Disorder**: The presence or absence of a sleep disorder in the person (None, Insomnia, Sleep Apnea, stored as string).

To download the dataset, click [here](https://www.kaggle.com/code/jillanisofttech/sleep-health-and-lifestyle-predication-with-94-ac).

With this dataset, we aim to predict a person's sleep quality. We chose this response variable as we think it is valuable information for a person. Sleep quality is important for your health and can affect mental-health issues. It also influences how you feel during the day, which affects the activities you undertake and how well you do them.

# Data cleaning
Before we begin, the data should be cleaned for proper use.

## Missing data
```{r}
md.pattern(sleepdata, rotate.names = TRUE)
```
There is no missing data in this dataset.

## Transform variable types
Let's convert the variables to the right type for exploration and prediction later on. We chose to transform the sleep quality (a scale from 1 - 10) to a factor, as we have found in literature that for ratings with a narrow range, classification techniques often work better than regression.[1](https://aclanthology.org/W10-1205.pdf), [2](https://towardsdatascience.com/1-to-5-star-ratings-classification-or-regression-b0462708a4df). Blood.Pressure is split into Systolic (pressure) and Diastolic (pressure), both are then compared to ideal/high/low pressure and labeled as such.[3](https://www.nhs.uk/common-health-questions/lifestyle/what-is-blood-pressure/) 
```{r}
# Check how many unique values of occupation, to see if it makes sense to convert to factor, or if there are too many unique values
unique(sleepdata$Occupation)
#The amount of values for the other chr variables were inspected in the R environment

# All variables are changed to factor, as downloading a .csv changes all properties
sleepdata %<>% 
  mutate(Gender   = as.factor(Gender),
         Occupation = as.factor(Occupation),
         BMI.Category = as.factor(BMI.Category),
         Sleep.Disorder = as.factor(Sleep.Disorder),
         Sleep.Disorder = as.factor(Sleep.Disorder),
         Quality.of.Sleep = as.factor(Quality.of.Sleep)) 

# Blood.Pressure is split, converted, and compared
sleepdata %<>% 
  separate_wider_delim(cols = Blood.Pressure, delim = "/", names = c("Systolic", "Diastolic")) %>% 
  mutate(Blood.Reading = ifelse(
    test = Systolic >= 130 & Diastolic >= 90, yes = "High", no = ifelse(
      test = Systolic <= 90 & Diastolic <= 60, yes = "Low", no = "Ideal"
    ))) %>% 
  mutate(Systolic = as.numeric(Systolic)) %>% 
  mutate(Diastolic = as.numeric(Diastolic)) %>% 
  mutate(Blood.Reading = as.factor(Blood.Reading))
```

## Select variables
For our analysis and prediction we do not need the person identifier of each case, so we remove this variable.
```{r}
sleepdata %<>% select(-Person.ID) # Remove Person.ID from sleepdata
```

## Outlier removal
We checked for potential outliers based on the IQR criterion. For this, we used boxplot stats per numerical value, and looked how many outliers it produced. 

```{r, warning=FALSE}
# Check the outliers per variable:
boxplot.stats(sleepdata$Sleep.Duration)$out 
boxplot.stats(sleepdata$Quality.of.Sleep)$out
boxplot.stats(sleepdata$Physical.Activity.Level)$out
boxplot.stats(sleepdata$Stress.Level)$out
boxplot.stats(sleepdata$Heart.Rate)$out
boxplot.stats(sleepdata$Daily.Steps)$out

out <- boxplot.stats(sleepdata$Heart.Rate)$out # Save the outliers of Heart.Rate in variable out
out_ind <- which(sleepdata$Heart.Rate %in% c(out)) # Find the indices of the outliers in the dataset
sleepdata[out_ind, ] %>% 
  select(Heart.Rate, BMI.Category, Sleep.Disorder) # Show the outliers in the dataset
```
As can be seen, only the Heart.Rate variable contains outliers; people with a much higher than average heart rate during sleep. When we look at the entries, we see that all these people have a sleep disorder, and that almost all of them are overweight or obese. Because of this, we assume that these are "expected" outliers, and keep them in the dataset. 

# Exploratory data analysis

To explore the data, we take a look at different factors we have deemed as important: Quality of sleep, sleep duration, and BMI category

```{r}
# Create a boxplot of the sleep quality per sleep duration
plot_sleepdata2 <- sleepdata %>% mutate(
  Quality.of.Sleep = as.factor(Quality.of.Sleep)
) %>% 
  ggplot(aes(x=Quality.of.Sleep, y=Sleep.Duration)) + 
  geom_boxplot(fill = "lightblue") + 
  theme_minimal() +
  labs(title="Sleep quality compared to sleep duration", x="Sleep quality", y="Sleep duration")
plot_sleepdata2
```

As can be seen in the box plot above, generally, the sleep quality increases as sleep duration increases. 
```{r}
ggplot(sleepdata, aes(x = Gender, y = as.numeric(Quality.of.Sleep))) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Sleep Quality by Gender", y = "Quality of Sleep")
```

Moreover, women seem to have a higher quality of sleep on average compared to men. To compare the quality of sleep to itself in a boxplot, it has to be converted back to a numeric value. 

```{r}
sleepdata %>% 
  group_by(Occupation) %>%
  summarise( 
    median = median(Sleep.Duration),
    min = min(Sleep.Duration),
    max = max(Sleep.Duration)
  )
```

Then, looking at the median, min, max, and median absolute deviation sleep duration per occupation, it can be seen that Sales Representatives sleep the least on average, followed by Scientist. Though the variation is not very strong per occupation, there is some variance. 

```{r}
sleepdata %>% 
  ggplot(aes(x = BMI.Category, y = as.numeric(Quality.of.Sleep))) + 
  scale_x_discrete(limits = c("Normal", "Normal Weight", "Overweight", "Obese")) +
  geom_boxplot(fill = "lightblue") + 
  theme_minimal() + 
  labs(title = "Sleep Quality per BMI category", x = "BMI category", y = "Quality of Sleep")
```

Lastly, looking at how the BMI category impacts the quality of sleep, people with lower weight seem to sleep better than heavier people. Again, to compare quality of sleep to itself in a boxplot, it has to be converted back to a numeric value.

# Prediction model
As described above, the response variable we chose to predict is quality of sleep. This is a (transformed) categorical variable with possible values 1 through 10.

## Simple model selection
As a model for this prediction problem we considered decision trees (including bagging, boosting, and random forest), support vector machines, LDA, and K-nearest neighbors, since these are the methods we studied in the course for multi-class classification.

We decided not to go with support vector machines as it is most effective when there is a clear margin of separation between classes, and we don't think this will be the case for this dataset. Although we transformed the response variable to a factor, we know there is actually an ordinal structure in the variable. The ratings are also subjective, so it could for example be that some people rated their sleep quality a 6, while others would rate it 5 or 7. This means the classes are probably overlapping.

This is also the reason why we do not think KNN will be a good method, as this method works best for well-defined clusters.

LDA is an effective method for multi-class identification, but assumes that the features are normally distributed within each class and that the covariance matrices of different classes are equal. As categorical variables are not continuous and therefore cannot be normally distributed, so we do not use this method.

We chose to use decision trees as classification models as they are robust, handle non-linear relationships well, and can capture complex interactions in the data. They is also good at finding the most informative predictors in the set without having to specify them. As a first model, we simply train one decision tree. 

### Split train test
First we split the data in training, and test data. For tree generation, we remove Diastolic and Systolic from the data, as we have a new variable (Blood.Reading) in which these values are combined. 
```{r}
set.seed(123)

sleepdatatree <- sleepdata %>% 
  select(-Diastolic, -Systolic)

# Create a training set and a test set
train <- sample(1:nrow(sleepdatatree), 0.75*nrow(sleepdatatree)) # 75% of the rows for the training set
train_data <- sleepdatatree[train,]
test_data <- sleepdatatree[-train,] # Use the other 25% of the rows for the test set

```

### Train tree model
Then, we train the decision tree model using the training data.

```{r}
# Train the decision tree model using the training data
tree_mod <- rpart(Quality.of.Sleep ~ ., data = train_data, method = "class") # method = "class" because we are predicting a categorical variable

# Plot tree
rpart.plot(tree_mod, yesno=2) # Yes/no at each split

```

As can be seen, the sleep quality levels of 4 and 5 are not predicted in the tree. However, a sleep quality of 9 is predicted correctly in all cases. And the sleep quality of 7 is in most cases incorecctly predicted (for instance in 43% of the cases, a 4 is the true data, but classified as a 7). Moreover, the tree does not use all variables; only Stress.Level, Occupation, and Sleep.Duration are used (3 out of 11 variables). This means that these variables are the strongest contributors to predicting Quality.of.Sleep. 

By letting the splits continue until all cases are classified, we could generate a tree with all predictors. However, this model will have too much variance to perform well on new samples (imposing a unfavourable bias/variance tradeoff). So we disregard that model, as it is likely to overfit the data. 

### Performance evaluation
Now, we can use the tree model to predict the sleep quality of the test data.

```{r}
# Class prediction
class_predicted <- predict(tree_mod,
                           newdata = test_data,
                           type = "class")
class_predicted

# Generate confusion matrix for the test data
confusionMatrix(class_predicted, reference = test_data$Quality.of.Sleep)
```

The accuracy of the model is 0.8723. The model is good at predicting a sleep quality of 9, but not at all at predicting a sleep quality of 4 or 5. 

The next step is to try to improve the model. We will try to improve the model by tuning the parameters. Gini-splitting and information-splitting are two different ways to measure the impurity of the nodes. As gini-splitting is the default, we are now rebuilding the decision tree using information-splitting to compare the accuracy.

```{r}
# Model accuracy
accuracy(actual = test_data$Quality.of.Sleep, predicted = class_predicted) 

# Model training based on information splitting.
tree_mod_info <- rpart(Quality.of.Sleep ~ .,
                       data = train_data, 
                       method = "class", 
                       parms = list(split = "information")) # split = information means that the split is based on the information gain of the nodes.

# Class prediction with information-based splitting
pred2 <- predict(tree_mod_info,
                 newdata = test_data, 
                 type = "class")
```


The data of the classification accuracy on the test data will be compared for the two models. 
```{r}
# Compare classification accuracy on test data
accuracy(actual = test_data$Quality.of.Sleep, predicted = pred2) 

rpart.plot(x = tree_mod_info, yesno = 2)
```

The accuracy of the model based on gini-splitting is 0.8723, and the accuracy of the model based on information-splitting is 0.9149. The model based on information-splitting is better at predicting the sleep quality of the test data. The above tree still does not classify level 4 and 5, but it does add an additional predictor (Heart.Rate), while removing the predictor Stress.Level.

As the tree is relatively small and not overly complex, we do not prune it.

# Complex model selection
Our simple model, a single decision tree, is prone to overfit on the training data. To improve upon this model, we tried random forest and boosting, as these are ensemble methods that combine predictions of multiple trees. This helps reduce overfitting by aggregating the results of multiple models, leading to more robust generalization to unseen data. This also means they are less sensitive to noise and outliers in the data. 
Decision trees might not always effectively prioritize and select the most important features, as they work top-down and are greedy. Random Forest and boosting methods provide a more reliable measure of feature importance by considering the collective contribution of features across multiple trees.
Decision trees also may struggle with imbalanced datasets, favoring the majority class.
While boosting methods in particular can assign higher weights to misclassified instances, helping address class imbalances more effectively.
So overall, Random Forest and Boosting have many advantages compared to decision trees.

## Random forest
Let's train a random forest model on the training data, and predict the labels of the test data.
```{r}
cvcontrol <- trainControl(method = "repeatedcv", # Cross validation
                          number = 10,
                          allowParallel = TRUE)
rf_train <- train(Quality.of.Sleep ~ .,
                  data = train_data, 
                  method = 'rf',
                  trControl = cvcontrol,
                  importance = TRUE)

rf_test  <- predict(rf_train, newdata = test_data)
```

As the structure information of the trees is lost when averaging multiple trees, we can inspect which predictors were most informative by plotting the variable importance. The variable importance was returned per level of the response variable, so the values were averaged over the levels. 

```{r}
data.frame(varImp(rf_train)$importance) %>% 
  mutate(avg_Importance = rowMeans(.), names=row.names(.)) %>% # Mean of varImp for each response level
  ggplot(aes(x=reorder(names, avg_Importance), y=avg_Importance)) + 
  geom_bar(stat = "identity", fill = "lightblue") + 
  coord_flip() + 
  theme_minimal() + 
  labs(title="Variable importance plot", x = "Average importance", y = "Names of predictors")
```

The most important predictors for the random forest model are sleep duration and stress level. This is the same as the predictors used in the single decision tree.

### Evaluation random forest
To evaluate whether this model is better than the simple model, we create a confusion matrix again and calculate relevant metrics.
```{r}
confusionMatrix(rf_test, test_data$Quality.of.Sleep)
```
The performance of the random forest model is very good, with an accuracy of 96.81% on the test data. The sensitivity and specificity are high as well, as can be seen in the statistics of the confusion matrix. 

## Boosting
A boosting model is different from Random Forest, it builds a decision tree sequentially, based on the residuals from each previous model. Let's see if a boosting model performs even better.
```{r, warning=FALSE}
gbm_train <- train(Quality.of.Sleep ~ .,
                   data = train_data,
                   method = "gbm",
                   verbose = F, 
                   trControl = cvcontrol)

summary(gbm_train, plotit = FALSE) # Var importance plot
```

Sleep duration and stress level are again the most important predictors.

### Evaluation boosting
```{r}
gbm_test <- predict(gbm_train, newdata = test_data)
confusionMatrix(gbm_test, test_data$Quality.of.Sleep)
```

The boosting model is only slightly better than the Random Forest model, the accuracy is 0.9787. The difference is so small that it is probably just random variation.

# Conclusions

As can be seen by the high accuracy, decision trees are a good type of classification model for predicting the sleep quality in this dataset. We found the best results for the boosting model, which returned 0.9787 accuracy, compared to 0.9681 for the random forest. The single decision tree returned 0.8723 accuracy. The advantages we mentioned before for random forest and boosting hold for this dataset, as the prediction accuracy improved quite significantly. 

In the exploratory data analysis, we expected sleep duration and BMI category would be strong predictors for quality of sleep. This turned out to be mostly true, sleep duration was the most important while BMI was not significantly important. Stress level and age were additional important predictors. 

## Acknowledgement 
Sadly, after all our analyses, we found out that the data is synthetic and created by the author for illustrative purposes. However, the performed analyses should still hold as if the data was genuine. Any patterns that we found however, are most likely not present in actual sleep/health data. 


