---
title: "Samenvatting practicals"
author: "Mirre Dona"
date: "1-11-2023"
output:
  word_document: default
  html_document: default
  pdf_document: default
fontsize: 12pt
urlcolor: blue
mainfont: Arial
---
# libraries
```{r}
#use always:
library(ISLR)
library(tidyverse)
#for reading in data:
library(haven)
library(readxl)

library(magrittr) #to use piping %>% 
library(class) #knn()
library(MASS) #lda()
library(splines) #bs() ns()
library(cowplot) #plot_grid()
library(psych) #describeBy()
#tree based methods
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC) #ROC curve
library(caret) #bagging,  boosting, rf

#datasets
library(DAAG) #elastic
library(mice) #boys
library(MASS) #Boston & Default
```

# Loading data
```{r}
#from csv
dataset <- read.csv('data/googleplaystore.csv')
data2 <- read.csv(url("https://..."))
#from excel
students <- read_excel('data/students.xlsx')
```

#creating data
```{r}
#create dataframe:
df<- data.frame(col1 = ..., col2=...)

#select column:
df$col1
df[ , 1]
#select row or rows
df[2, ]
df[1:10, ]

#example to create tibble:
student_grade  <- rnorm(32, 7) #length 32, random values from normal distr, mean of 7 and stddev of 1
student_number <- round(runif(32) * 2e6 + 5e6) #32 random numbers from uniform distribution between 0
  #and 1, then make sure all end up in range between 5mil and 7mil
programme      <- sample(c("Science", "Social Science"), 32, replace = TRUE) #32 random program choices

gg_students <- tibble(
  number = as.character(student_number),
  grade  = student_grade,
  prog   = as.factor(programme)         
)
```

#missing data
```{r}
#inspect missing data pattern, will show kind of heatmap (see practical 3 for interpretation)
md.pattern(boys)
#create missingness indicator, in this example for var gen
boys_mis <- boys %>%
  mutate(gen_mis = is.na(gen))
#check if missingness in one var related to other var
boys_mis %>%
  group_by(gen_mis) %>%
  summarize(age = mean(age))

#remove missing data
baseball <- Hitters %>% filter(!is.na(Salary))
```

# transform data
```{r}
#filter: selects rows from a data frame
filter(students, grade < 5.5, programme == "A")
#filter missing values
filter(!is.na(column))
#arrange: sorts dataframe by 1 or more columns, default asc and with - desc
arrange(students, programme, -grade) 
#select: selects columns of interest
select(students, student_number, programme)
#mutate: compute new columns and transform existing columns
students <- mutate(students, programme = recode(programme, "A" = "Science", "B" = "Social Science"))

#pivot longer: increasing the number of rows and decreasing the number of columns
#dataset with cols country, 1999, 2000, 2001, 2002. the values for the year columns are nr of cases of a disease. convert to cols country, year, cases
data %>% pivot_longer( 
  cols = 1999:2002,
  names_to = "year",
  values_to = "cases"
)

#pivot wider: increasing the number of columns and decreasing the number of rows. other way around
data %>% pivot_wider(
  names_from = "year",
  values_from = "cases"
)
```

# Grouping and summarisation
```{r}
#create summary
summary(dataset) 

#grouping data
groups <- group_by(students, programme)

#summary statistics > works with any function that takes a vector of numbers and outputs a single number
groups %>% 
  summarise(
    mean = mean(grade), 
    variance = var(grade), 
    min = min(grade), 
    max = max(grade)
  )

#other descriptive statistics, based on different level response variable 'disease'. To check whether differences in data based on this var
df <- readRDS("data/train_disease.RDS")
df %>% describeBy(df$Disease, fast = TRUE)
```

# --- Data visualization ---
examples of aesthetics are: x, y, alpha (transparency), colour, fill, group, shape, size, stroke
```{r}
#histogram: for continuous data, divides in intervals/bins, bar height means frequency/density of data points within that interval
students %>%
  ggplot(aes(x = grade)) +
  geom_histogram(binwidth = .5) + theme_minimal()

#barplot: useful for categorical data with discrete groups, automatically transforms variables to counts
Hitters %>% 
  ggplot(aes(x = Years)) + 
  geom_bar() +    #use geom_col if count already in data
  theme_minimal()

#density plot:
students %>% 
  ggplot(aes(x = grade)) +
  geom_density(fill = "light seagreen", colour = NA) + #density = abstraction from the raw data, thus it     might alter interpretations. E.g. it could be that a grade between 8.5 and 9 is in fact impossible.
  geom_rug(size = 1, colour = "light seagreen") + #add raw data in form of rugs 
  xlim(0, 10)

#boxplot: allows for visual comparison of the distribution of two or more groups through their summary statistics. see practical 2 for explanation of boxplot
gg_students %>% 
  ggplot(aes(x = prog, y = grade, fill = prog)) +
  geom_boxplot()

#lineplot: geom_line(), trekt lijn tussen datapunten. use size to increase line thickness

#geom_smooth(): add an automatically computed regression line
elastic %>%
  ggplot(aes(x = stretch, y = distance, col = Set)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Facets:
Use `facet_wrap(~column)`
Faceting should preferably be done using a factor variable. The order of the facets is taken from the levels() of the factor. Changing the order of the facets can be done using `fct_relevel()`.

```{r}
#mutliple plots in a grid
plot1 <- ggplot() + bla
plot2 <- ggplot() + bla
plot_grid(plot1, plot2)
```

Colours:
Use continuous colour scale if variable mapped to colour is numeric, e.g. `scale_colour_viridis_c()`
For a discrete variable for fill: `scale_fill_brewer()`
Can also create your own colour scales:
- discrete data: `scale_color_manual(values = c("A" = "red", "B" = "blue"))`
- continuous data: `scale_color_continuous(low = "blue", high = "red")`
- same works for fill, then it's `scale_fill_*()`

# --- Statistical models ---

Don't forget to convert categorical data to factors when using them in models! Like so:
```{r}
treat <- data %>% 
  mutate(severity = as.factor(severity),
         gender   = as.factor(gender),
         dose     = as.factor(dose),
         response = as.factor(response))
```

All models expect a formula in the form y ~ x, example of more complex formula: 
response ~ age + I(age^2) + gender + bb_score * prior_cvd * dose

- The * define interactions, they capture the combined effect of these variables on the response. Means that the effect of one of these variables may depend on the levels or values of the other two variables

#linear regression
```{r}
# lr_model <- lm(formula, data), e.g.
fit1 <- lm(distance ~ stretch, elastic1)

#check fitted values (predictions)
#predict(lr_model), e.g.
fit1 %>% predict(se.fit = TRUE)  #se.fit = TRUE prints the standard errors of the fitted values
#predict(lr_model, newdata=...) to make predictions on unseen data

#check residuals, coefficients, and proportion explained variance R^2 (multiple R squared)
summary(fit1)
#coef(model) gives just the coefficients

#plot residual vs leverage plot, cook's distance
fit1 %>% plot(which = 5)

#compute correlation between observed and predicted values
cor(observed, predicted)

#use geom_line() to plot line of predicted y values for the input x values
x_pred <- seq(min(Boston$lstat), max(Boston$lstat), length.out = 500)
y_pred <- predict(model, newdata = tibble(lstat = x_pred))
geom_line(data = tibble(lstat = x_pred, medv = y_pred))

#compute confidence interval for predicted values, results in a matrix with an estimate and a lower and an upper confidence interval for each point
y_pred_95 <- predict(model, newdata = pred_dat, interval = "confidence") #interval = "prediction" for PI

#plot confidence interval
gg_pred <- tibble(        #create dataframe with x, y, lower bound, upper bound
  lstat = pred_dat$lstat,
  medv  = y_pred_95[, 1],
  lower = y_pred_95[, 2],
  upper = y_pred_95[, 3]
)
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), data = gg_pred, fill = "#00008b44") +
  geom_point() + 
  geom_line(data = gg_pred)
#The ribbon represents the 95% confidence interval of the fit line. The uncertainty in the estimates of the coefficients are taken into account with this ribbon.
```

#KNN
```{r}
#knn(train, test, cl, k), cl are the class labels and k nr of nearest neighbours. the function trains on the train data and output predictions for test data
knn_5_pred <- knn(
  train = default_train %>% select(-default),
  test  = default_test  %>% select(-default),
  cl    = as_factor(default_train$default),
  k     = 5
)
```

#logistic regression
predicts the log-odds of belonging to category 1. These log-odds can then be transformed to probabilities by performing an inverse logit transform. So it outputs a probability which can then be used in conjunction with a cutoff (usually 0.5) to classify new observations.
```{r}
lr_mod <- glm(default ~ ., family = binomial, data = default_train) # indicate that the residuals are modeled not as a gaussian but as a binomial distribution

#By default predict outputs the log-odds, but we can transform it to probabilities using the inverse logit function of before or setting the argument type = "response"
pred_prob <- predict(lr_mod, type = "response")
#predicted labels:
pred_lr <- factor(pred_prob > .5, labels = c("No", "Yes"))

#plot using geom_line()
balance_df %>% 
  ggplot(aes(x = balance, y = predprob)) +
  geom_line(col = "dark blue", size = 1)
```

#LDA
```{r}
lda_mod <- lda(default ~ ., data = default_train)
#can inspect model result for coefficients, prior probabilities, group means

pred_lda <- predict(lda_mod)$class
```

#Classification trees
```{r}
tree_mod <- rpart(Species ~ ., data = iris)
rpart.plot(iris_tree_mod) #plot tree

#keep splitting until all observations classified
tree_full_mod <- rpart(Species ~ ., data = iris, control = rpart.control(minbucket = 1, cp = 0)) #minbucket specifies minimum number of observations required in leaf. cp controls complexity, value of 0 means tree may be as complex as can be, no pruning

#plot splits form tree as lines in scatterplot:
iris %>% 
  ggplot(aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
  geom_point() +
  geom_segment(aes(x = 2.5, xend = 2.5, y = -Inf, yend = Inf),
               colour = "black") +
  geom_segment(aes(x = 2.5, xend = Inf, y = 1.75, yend = 1.75), 
               colour = "black") +
  scale_colour_viridis_d()

#bagging
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)
bag_train <- train(Disease ~ .,
                   data = df, 
                   method = 'treebag',
                   trControl = cvcontrol,
                   importance = TRUE)
predictions <- predict(bag_train)
  
bag_train %>% #plot variable importance
  varImp %>%
  plot

#random forest
rf_mod <- randomForest(Species ~ ., data = iris)
predicted_labels <- predict(rf_mod, newdata = iris)
predicted_probabilities <- predict(rf_mod, newdata = iris, type = "response")

#variable importance (practical6)
var_imp <- importance(rf_mod)

tibble(
  importance = c(var_imp), 
  variable = rownames(var_imp)
) %>% 
  ggplot(aes(x = variable, y = importance, fill = variable)) +
  geom_bar(stat = "identity")

#random forest with caret package
rf_train <- train(Disease ~ .,
                  data = df, 
                  method = 'rf',
                  trControl = cvcontrol,
                  importance = TRUE)
predictions <- predict(rf_train)

#boosting
gbm_train <- train(Disease ~ .,
                   data = df,
                   method = "gbm",
                   verbose = F, 
                   trControl = cvcontrol) #print bag_train, rf_train, gbm_train models to see accuracy
summary(gbm_train) #var importance plot
predictions <- predict(gbm_train)

#See practical 8 for xgboost and SHAP values
```

#nonlinear regression 
Polynomial regression
```{r}
pn3_mod <- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data = Boston) #or
pn3_mod2 <- lm(medv ~ poly(lstat, 3, raw = TRUE), data = Boston) #raw = FALSE is orthogonal (uncorrelated) polynomial. if true can have predictor variables that are highly correlated, which can lead to unstable coefficient estimates and difficulty in interpreting the model

#plot line using geom_line, create new dataset with many values for x and the predictions from model
```

Piecewise regression
```{r}
pw5_mod <- lm(medv ~ cut(lstat, 5), data = Boston) #5 equally spaced sections in terms of lstat
#sections with equal amount of training data
brks <- c(-Inf, quantile(Boston$lstat, probs = c(.2, .4, .6, .8)), Inf)
pwq_mod <- lm(medv ~ cut(lstat, brks), data = Boston)
```

Piecewise polynomial regression, see practical 7 for function 'piecewise_cubic_basis()'

Splines
```{r}
#cubic spline model with knot at median
bs1_mod <- lm(medv ~ bs(lstat, knots = median(lstat)), data = Boston)
#natural spline, function is required to be linear at the boundaries
ns3_mod <- lm(medv ~ ns(lstat, df = 3), data = Boston) #df = number of basis functions used (a combination of piecewise polynomial functions, with knots that divide the range of the predictor variable)
```

# --- performance measure ---
# train, dev, test split
```{r}
df %>% mutate(split = sample(rep(c("train", "test"), times = c(0.8*nrow, 0.2*nrow))))
df_train <- df %>% 
  filter(split == "train") %>% 
  select(-split)
```

#measures
```{r}
#MSE
mse <- function(y_true, y_pred) mean((y_true - y_pred)^2)
```

```{r}
#confusion matrix
cmatrix <- table(true = true_labels, predicted = predicted_labels)
```

```{r}
#compute metrics from confusion matrix, wikipedia confusion matrix: https://tinyurl.com/mtwd6xwn
TN <- cmatrix[1, 1]
FN <- cmatrix[2, 1]
FP <- cmatrix[1, 2]
TP <- cmatrix[2, 2]

tibble(
  Acc = (TP + TN) / sum(cmat_lr),
  TPR = TP / (TP + FN),
  TNR = TN / (TN + FP),
  FPR = FP / (TN + FP),
  PPV = TP / (TP + FP),
  NPV = TN / (TN + FN)
)

#function to automatically compute cmatrix and all metrics
confusionMatrix(predictions, true_labels)
```

```{r}
#Brier score
#score 0 = perfect accuracy and score 1 = perfect inaccuracy. It's the mean squared difference between the probability and the true class
mean((prob_lr - (as.numeric(data$true_label) - 1)) ^ 2)   #true labels are 0 or 1
```

```{r}
#ROC curve, plots on x-axis specificity and on y-axis sensitivity for every possible discrimination threshold (cut-off point for the pred prob to decide the labels), works for binary classification methods like lr
roc_lr <- roc(treat$response, prob_lr) #print object to find AUC
ggroc(roc_lr) + theme_minimal()
```

# -- miscellaneous --
Example explanation about plot: (from practical 2)

 Aesthetics: number of hits mapped to x-position, number of home runs mapped to y-position
 Geoms: points and contour lines
 Scales: x-axis: continuous, y-axis: continuous
 Facets: None
 Statistical transformations: None
 Special coordinate system: None (just cartesian)
 
```{r}
#create dummy variable (0 or 1) of cateogirical data for use in model. important for variables with more categories, where there is no direct order. simply converting to numeric like 1,2,3 suggests order. Should create n-1 dummy variables where n = nr of categories
mutate(student = ifelse(student == "Yes", 1, 0))
```

```{r}
#use of cut(): e.g. create a factor variable which splits playersâ€™ salary range into 3 categories
Hitters %>% 
  filter(!is.na(Salary)) %>% 
  mutate(
    Salary_range = cut(Salary, breaks = 3, 
                       labels = c("Low salary", "Mid salary", "High salary")))
```

```{r}
#Use of seq(): e.g. generate a sequence of 1000 equally spaced values from 0 to 40
seq(0, 40, length.out = 1000) 
#can use as new data for statistical model predictions, e.g for Boston dataset
pred_dat <- tibble(lstat = seq(0, 40, length.out = 1000))
```

```{r}
#Test whether data is sorted on a variable
!is.unsorted(boys$age)
```

```{r}
#bind dataframes together
#with same columns, add identifier col called Set
elastic <- bind_rows("Elastic1" = elastic1,
                     "Elastic2" = elastic2,
                     .id = "Set")
#with same nrow
bind_cols(default_test, pred = knn_5_pred) #this example binds training data with class labels
```
